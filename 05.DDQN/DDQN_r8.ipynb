{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# package, def load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:36.208657Z",
     "start_time": "2022-11-23T06:57:33.812376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import random \n",
    "import math\n",
    "from pandas import DataFrame\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import _pickle as pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:36.213048Z",
     "start_time": "2022-11-23T06:57:36.209867Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def my_save(List,filename,Global):\n",
    "    # filename : name.pkl\n",
    "    my_pickle = {}\n",
    "    for key in List:\n",
    "        try:\n",
    "            my_pickle[key] = Global[key] #globals()[key]\n",
    "        except TypeError:\n",
    "            print('ERROR pickleing: {0}'.format(key))\n",
    "    with open(filename, 'wb') as f:   # Python 3: open(..., 'wb')\n",
    "        pickle.dump(my_pickle, f)\n",
    "\n",
    "\n",
    "def my_load(filename,Global):\n",
    "    with open(filename, 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        my_shelf = pickle.load(f)\n",
    "\n",
    "    for key in my_shelf:\n",
    "        Global[key] = my_shelf[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T02:55:21.082720Z",
     "start_time": "2022-07-11T02:55:21.080521Z"
    },
    "collapsed": true
   },
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:45.112706Z",
     "start_time": "2022-11-23T06:57:36.214228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_data.csv', 'train_data.csv', 'val_data.csv', 'test_data.csv']\n",
      "(12, 273)\n"
     ]
    }
   ],
   "source": [
    "dirname = \"S_R8/\"\n",
    "mimic_dir = \"/public/hanl/jupyter_dir/ir_recommend_v2/04.merge/\" + dirname\n",
    "print(os.listdir(mimic_dir))\n",
    "\n",
    "df_train = pd.read_csv(mimic_dir + '/train_data.csv')\n",
    "#df_train.loc[(df_train[\"reward\"]==-40),\"reward\"] = -100 # 163条\n",
    "df_val = pd.read_csv(mimic_dir + '/val_data.csv')\n",
    "df_test = pd.read_csv(mimic_dir + '/test_data.csv')\n",
    "\n",
    "df_all = pd.read_csv(mimic_dir + '/all_data.csv')\n",
    "df_all = df_all.sort_values([\"traj_id\",\"starttime\"])\n",
    "df_all[\"last_rate\"].fillna(0,inplace=True)\n",
    "\n",
    "df_train = df_train[df_train[\"idx\"] != max(df_all[\"idx\"])]\n",
    "\n",
    "df_train.index = pd.RangeIndex(len(df_train.index))\n",
    "df_val.index = pd.RangeIndex(len(df_val.index))\n",
    "df_test.index = pd.RangeIndex(len(df_test.index))\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter setting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data parapmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:45.116834Z",
     "start_time": "2022-11-23T06:57:45.113994Z"
    }
   },
   "outputs": [],
   "source": [
    "no_state_features = [ 'stay_id',\n",
    "                     'traj_id','heart_rate_mean.1', 'resp_rate_mean.1',\n",
    " 'reward',\"idx\",\n",
    " 'rate', 'rate_bin',\n",
    " 'next_glucose_max', 'next_glucose_min','next_glucose_mean'\n",
    "]\n",
    "state_features = list(set(df_train.columns.tolist()) - set(no_state_features))\n",
    "reward_features = ['reward']\n",
    "action_features = [\"rate_bin\"]\n",
    "\n",
    "REWARD_THRESHOLD_max = 21\n",
    "REWARD_THRESHOLD_min = -41\n",
    "REWARD_THRESHOLD = max(abs(REWARD_THRESHOLD_max), abs(REWARD_THRESHOLD_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- buffer reply\n",
    "- https://github.com/jcborges/dqn-per/blob/master/DQN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get SAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## action to bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.064298Z",
     "start_time": "2022-11-23T06:57:45.117694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b5a27fdc400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(df_train[\"rate\"])\n",
    "sns.distplot(df_train[\"rate_bin\"])\n",
    "plt.legend([\"rate\", \"rate_bin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.070970Z",
     "start_time": "2022-11-23T06:57:47.065269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 这块要求dataframe 按时间从小到大排序\n",
    "def process_train_batch(size, df):\n",
    "    df_all_ = df_all.copy()\n",
    "    df = df[df[\"idx\"] != max(df_all_[\"idx\"])]\n",
    "    if geometric_sampling == True:\n",
    "        a = df.sample(n=size, weights = df.groupby('rate_bin')['rate_bin'].transform('count'))\n",
    "    else:\n",
    "        a = df.sample(n=size)\n",
    "    states = None\n",
    "    actions = None\n",
    "    rewards = None\n",
    "    next_states = None\n",
    "\n",
    "    for i in a.index:\n",
    "        cur_state = a.loc[i,state_features]\n",
    "        action = a.loc[i,'rate_bin']\n",
    "        reward = a.loc[i,'reward']\n",
    "        traj = a.loc[i,\"traj_id\"]\n",
    "        \n",
    "        idx_num = a.loc[i,\"idx\"]\n",
    "        \n",
    "        if df_all_.loc[idx_num, 'traj_id'] == df_all_.loc[idx_num+1, 'traj_id']:\n",
    "            next_state = df_all_.loc[idx_num+1, state_features]\n",
    "        else:\n",
    "            next_state = np.zeros(len(cur_state))\n",
    "                \n",
    "        if clip_reward:\n",
    "            if reward > REWARD_THRESHOLD_max: reward = REWARD_THRESHOLD_max\n",
    "            if reward < REWARD_THRESHOLD_min: reward = REWARD_THRESHOLD_min\n",
    "\n",
    "        if states is None:\n",
    "            states = copy.deepcopy(cur_state)\n",
    "        else:\n",
    "            states = np.vstack((states,cur_state))\n",
    "\n",
    "        if actions is None:\n",
    "            actions = [action]\n",
    "        else:\n",
    "            actions = np.vstack((actions,action))\n",
    "\n",
    "        if rewards is None:\n",
    "            rewards = [reward]\n",
    "        else:\n",
    "            rewards = np.vstack((rewards,reward))\n",
    "\n",
    "        if next_states is None:\n",
    "            next_states = copy.deepcopy(next_state)\n",
    "        else:\n",
    "            next_states = np.vstack((next_states,next_state))\n",
    "    \n",
    "    return (states, np.squeeze(actions), np.squeeze(rewards), next_states, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.077828Z",
     "start_time": "2022-11-23T06:57:47.071880Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract chunks of length size from the relevant dataframe, and yield these to the caller\n",
    "def process_eval_batch(batch_size, eval_type = None):\n",
    "    df_all_ = df_all.copy()\n",
    "    if eval_type is None:\n",
    "        raise Exception('Provide eval_type to process_eval_batch')\n",
    "    elif eval_type == 'train':\n",
    "       # \n",
    "        a = df_train.copy()\n",
    "    elif eval_type == 'val':\n",
    "        a = df_val.copy()\n",
    "    elif eval_type == 'test':\n",
    "        a = df_test.copy()\n",
    "    else:\n",
    "        raise Exception('Unknown eval_type')\n",
    "    count = 0\n",
    "    while count < len(a.index):\n",
    "        states = None\n",
    "        actions = None\n",
    "        rewards = None\n",
    "        next_states = None\n",
    "\n",
    "        start_idx = count\n",
    "        end_idx = min(len(a.index), count+batch_size)\n",
    "        segment = a.index[start_idx:end_idx]\n",
    "        \n",
    "        for i in segment:\n",
    "            cur_state = a.loc[i,state_features]\n",
    "            action = a.loc[i, 'rate_bin']\n",
    "            reward = a.loc[i,'reward']\n",
    "            \n",
    "            idx_num = a.loc[i,\"idx\"]\n",
    "            if df_all_.loc[idx_num, 'traj_id'] == df_all_.loc[idx_num+1, 'traj_id']:\n",
    "                next_state = df_all_.loc[idx_num+1, state_features]\n",
    "            else:\n",
    "                next_state = np.zeros(len(cur_state))\n",
    "            \n",
    "            if clip_reward:\n",
    "                if reward > REWARD_THRESHOLD_max: reward = REWARD_THRESHOLD_max\n",
    "                if reward < REWARD_THRESHOLD_min: reward = REWARD_THRESHOLD_min\n",
    "\n",
    "            if states is None:\n",
    "                states = copy.deepcopy(cur_state)\n",
    "            else:\n",
    "                states = np.vstack((states,cur_state))\n",
    "\n",
    "            if actions is None:\n",
    "                actions = [action]\n",
    "            else:\n",
    "                actions = np.vstack((actions,action))\n",
    "\n",
    "            if rewards is None:\n",
    "                rewards = [reward]\n",
    "            else:\n",
    "                rewards = np.vstack((rewards,reward))\n",
    "\n",
    "            if next_states is None:\n",
    "                next_states = copy.deepcopy(next_state)\n",
    "            else:\n",
    "                next_states = np.vstack((next_states,next_state))\n",
    "\n",
    "        yield (states, np.squeeze(actions), np.squeeze(rewards), next_states,  a)\n",
    "        \n",
    "        count += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def for main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.091851Z",
     "start_time": "2022-11-23T06:57:47.079293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 262 14\n"
     ]
    }
   ],
   "source": [
    "num_actions = len(df_train[\"rate_bin\"].unique())\n",
    "state_dim = len(state_features)\n",
    "max_rate_bin = max(df_train[\"rate_bin\"])\n",
    "\n",
    "print(num_actions, state_dim, max_rate_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.102545Z",
     "start_time": "2022-11-23T06:57:47.092986Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "hidden_1_size = 200\n",
    "hidden_2_size = 100\n",
    "#  Q-network uses Leaky ReLU activation\n",
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        self.num_actions = num_actions\n",
    "        self.input_size = state_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, self.input_size],name=\"input_state\")\n",
    "        self.actions_max = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions_min = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        \n",
    "        self.fc_1 = tf.compat.v1.layers.dense(self.state, hidden_1_size, activation=None)\n",
    "        self.fc_2 = tf.compat.v1.layers.dense(self.fc_1, hidden_2_size, activation=None)\n",
    "        # advantage and value streams\n",
    "        self.streamA, self.streamV = tf.split(self.fc_2, 2, axis=1)\n",
    "        self.AW = tf.Variable(tf.random_normal([hidden_2_size//2, self.num_actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([hidden_2_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.q_output = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        #if probs > epsilon:\n",
    "        self.predict_fix = tf.argmax(self.q_output, 1, name='predict')\n",
    "       # else:\n",
    "        self.dims2 = tf.shape(tf.multinomial(tf.log(self.q_output),1))[0]\n",
    "       # self.predict_ran = tf.squeeze(tf.random_uniform((self.dims2, 1), minval=0, maxval= num_actions, dtype=tf.int32), axis=1)\n",
    "        self.predict_ran = tf.compat.v1.to_int32(tf.diag_part(tf.random_uniform((self.dims2, 1), minval=self.actions_min, maxval=self.actions_max,dtype=tf.float32)))\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and predicted Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,self.num_actions,dtype=tf.float32) # actions维度为batch_size*1 hotencoding后为32*29\n",
    "\n",
    "        # select the Q values for the actions that would be selected         \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_output, self.actions_onehot), reduction_indices=1) # 每个横轴上输出一个值\n",
    "        self.abs_error = tf.abs(self.targetQ - self.Q)\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.old_loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "    #    if per_flag:\n",
    "     #       self.loss = tf.reduce_mean(self.per_error) + reg_lambda*self.reg_term\n",
    "     #   else:\n",
    "        self.reg_vector = tf.maximum(tf.abs(self.Q)-REWARD_THRESHOLD,0) # regularisation penalises the network when it produces rewards that are above the reward threshold, to ensure reasonable Q-value predictions \n",
    "        self.reg_term = tf.reduce_sum(self.reg_vector)\n",
    "        self.loss = self.old_loss + reg_lambda*self.reg_term\n",
    "\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.update_model = self.trainer.minimize(self.loss) # 最小化loss\n",
    "\n",
    "# function is needed to update parameters between main and target network\n",
    "# tf_vars are the trainable variables to update, and tau is the rate at which to update\n",
    "# returns tf ops corresponding to the updates\n",
    "def update_target_graph(tf_vars,tau):\n",
    "    total_vars = len(tf_vars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tf_vars[0:int(total_vars/2)]):\n",
    "        op_holder.append(tf_vars[idx+int(total_vars/2)].assign((var.value()*tau) + ((1-tau)*tf_vars[idx+int(total_vars/2)].value())))\n",
    "    return op_holder\n",
    "\n",
    "def update_target(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.110102Z",
     "start_time": "2022-11-23T06:57:47.103485Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_eval(eval_type, batch_size):\n",
    "    gen = process_eval_batch(batch_size = batch_size, eval_type=eval_type)\n",
    "    iternum = 40\n",
    "    random_q_ret = []\n",
    "    phys_q_ret = []\n",
    "    actions_ret = []\n",
    "    agent_q_ret = []\n",
    "    actions_taken_ret = []\n",
    "    error_ret = 0\n",
    "\n",
    "    for b in gen:\n",
    "        probs = np.array(random.random()) \n",
    "        states,actions,rewards,next_states, _ = b\n",
    "        actions_min = actions - max_rate_bin\n",
    "        actions_min[actions_min<=0] = 0\n",
    "        actions_max = actions + max_rate_bin\n",
    "        actions_max[actions_max>=max_rate_bin] = max_rate_bin\n",
    "       # if probs > epsilon:\n",
    "        actions_from_q1 = sess.run(mainQN.predict_fix,feed_dict={mainQN.state:next_states, mainQN.phase : 0})\n",
    "        cur_act = sess.run(mainQN.predict_fix, feed_dict={mainQN.state:states, mainQN.phase : 0})\n",
    "       # else:\n",
    "       #     actions_from_q1 = sess.run(mainQN.predict_ran,feed_dict={mainQN.state:next_states, mainQN.phase : 0})\n",
    "            \n",
    "        Q1 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 0})\n",
    "        for j in range(1,iternum,1): \n",
    "            Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 0})\n",
    "            Q1 = np.dstack((Q1, Q2))\n",
    "        Q2 = np.mean(Q1, axis=2)\n",
    "        \n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(len(Q2)), actions_from_q1]\n",
    "      #  print(double_q_value.shape)\n",
    "        # empirical hack to make the Q values never exceed the threshold - helps learning\n",
    "      #  double_q_value[double_q_value > REWARD_THRESHOLD_max] = REWARD_THRESHOLD_max\n",
    "      #  double_q_value[double_q_value < REWARD_THRESHOLD_min] = REWARD_THRESHOLD_min\n",
    "        \n",
    "        \n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value)\n",
    "\n",
    "        # get the output q's, actions, and loss\n",
    "        q_output,actions_taken, abs_error = sess.run([mainQN.q_output, mainQN.predict_fix, mainQN.abs_error], \\\n",
    "                feed_dict={mainQN.state:states,\n",
    "                           mainQN.targetQ:targetQ, \n",
    "                           mainQN.actions:cur_act,\n",
    "                           mainQN.phase:False\n",
    "                          })\n",
    "        random_act = sess.run(mainQN.predict_ran, feed_dict={mainQN.state:states,  mainQN.actions_min:actions_min, mainQN.actions_max:actions_max, mainQN.phase : 0})\n",
    "        \n",
    "        random_q = q_output[range(len(q_output)), random_act]\n",
    "        phys_q = q_output[range(len(q_output)), actions.astype('int64')]\n",
    "        agent_q = q_output[range(len(q_output)), actions_taken] # actions_taken为mainQN.predict的结果\n",
    "        error = np.mean(abs_error)\n",
    "        \n",
    "        # update the return vals\n",
    "        phys_q_ret.extend(phys_q)\n",
    "        random_q_ret.extend(random_q)\n",
    "        actions_ret.extend(actions)        \n",
    "        agent_q_ret.extend(agent_q)\n",
    "        actions_taken_ret.extend(actions_taken)\n",
    "        error_ret += error\n",
    "  \n",
    "    return random_q_ret, phys_q_ret, actions_ret, agent_q_ret, actions_taken_ret, error_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.115942Z",
     "start_time": "2022-11-23T06:57:47.110993Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_save_results():\n",
    "    # get the chosen actions for the train, val, and test set when training is complete.\n",
    "    ran_q, phys_q_train, phys_actions_train, agent_q_train, agent_actions_train, _ = do_eval(batch_size=100, eval_type = 'train')\n",
    "#    print (\"length of training epoch \", len(agent_actions_train))\n",
    "  #  with open(save_dir + 'dqn_normal_actions_train.p', 'wb') as f:\n",
    "  #      pickle.dump(agent_actions_train, f)\n",
    "    ran_q, phys_q_val, phys_actions_val, agent_q_val, agent_actions_val, _ = do_eval(batch_size=100, eval_type = 'val')        \n",
    "    ran_q, phys_q_test, phys_actions_test, agent_q_test, agent_actions_test, _ = do_eval(batch_size=100, eval_type = 'test')   \n",
    "    \n",
    "    # save everything for later - they're used in policy evaluation and when generating plots\n",
    "    with open(save_dir + 'dqn_normal_actions_train.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_train, f)\n",
    "    with open(save_dir + 'dqn_normal_actions_val.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_val, f)\n",
    "    with open(save_dir + 'dqn_normal_actions_test.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_test, f)\n",
    "\n",
    "    with open(save_dir + 'dqn_normal_phy_actions_train.p', 'wb') as f:\n",
    "        pickle.dump(phys_actions_train, f)\n",
    "    with open(save_dir + 'dqn_normal_phy_actions_val.p', 'wb') as f:\n",
    "        pickle.dump(phys_actions_val, f)\n",
    "    with open(save_dir + 'dqn_normal_phy_actions_test.p', 'wb') as f:\n",
    "        pickle.dump(phys_actions_test, f)\n",
    "        \n",
    "    with open(save_dir + 'dqn_normal_q_train.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_train, f)\n",
    "    with open(save_dir + 'dqn_normal_q_val.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_val, f)\n",
    "    with open(save_dir + 'dqn_normal_q_test.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_test, f)\n",
    "    \n",
    "    with open(save_dir + 'dqn_normal_phy_q_train.p', 'wb') as f:\n",
    "        pickle.dump(phys_q_train, f)\n",
    "    with open(save_dir + 'dqn_normal_phy_q_val.p', 'wb') as f:\n",
    "        pickle.dump(phys_q_val, f)\n",
    "    with open(save_dir + 'dqn_normal_phy_q_test.p', 'wb') as f:\n",
    "        pickle.dump(phys_q_test, f)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T06:11:15.270530Z",
     "start_time": "2022-08-22T06:11:15.267164Z"
    }
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T06:57:47.119167Z",
     "start_time": "2022-11-23T06:57:47.116821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'av_loss_step = []\\nav_loss_lst = []\\n\\nav_q_step = []\\nav_agent_q_lst = []\\nav_phys_q_lst = []\\nav_phys_q_std_lst = []\\nav_agent_q_std_lst = []\\nav_ran_q_lst = []\\nav_ran_q_std_lst = []\\n    \\nM_phy_ac = []\\nE_phy_ac = []\\nEA_phy_ac = []\\nL_phy_ac = []\\n\\nerr_lst = [] '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"av_loss_step = []\n",
    "av_loss_lst = []\n",
    "\n",
    "av_q_step = []\n",
    "av_agent_q_lst = []\n",
    "av_phys_q_lst = []\n",
    "av_phys_q_std_lst = []\n",
    "av_agent_q_std_lst = []\n",
    "av_ran_q_lst = []\n",
    "av_ran_q_std_lst = []\n",
    "    \n",
    "M_phy_ac = []\n",
    "E_phy_ac = []\n",
    "EA_phy_ac = []\n",
    "L_phy_ac = []\n",
    "\n",
    "err_lst = [] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T11:03:28.341506Z",
     "start_time": "2022-11-23T06:57:47.120066Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-15ec97ea7262>:30: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:57:47\n",
      "Running default init\n",
      "Init done\n",
      "0.9 [ 0  1  3  6  7  9 10 13 14]\n",
      "0.9 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.9 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.9 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.9 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.9 [ 0  1  4  6  8 10 11 14]\n",
      "0.9 [ 0  2  3  4  6  8  9 12 13 14]\n",
      "0.9 [ 0  2  3  4  5  6  8  9 10 11 12 13 14]\n",
      "0.9 [ 1  2  4  5  6  7  8  9 11 12 13 14]\n",
      "0.9 [ 0  1  3  4  6  7  9 10 13 14]\n",
      "0.9 [ 0  1  2  3  5  6  7  8 10 11 12 13 14]\n",
      "2022-11-23 18:12:14\n",
      "10000 Saving results\n",
      "0.7 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14]\n",
      "0.7 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "2022-11-23 21:22:43\n",
      "20000 Saving results\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14]\n",
      "0.49999999999999994 [ 0  2  4  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  1  2  3  5  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.49999999999999994 [ 0  2  3  4  5  6  7  8  9 11 12 13]\n",
      "2022-11-24 00:29:09\n",
      "30000 Saving results\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  7  8 11]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.29999999999999993 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "2022-11-24 03:36:13\n",
      "40000 Saving results\n",
      "0.09999999999999992 [ 0  1  2  3  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  3  4  5  6  7  8  9 10 11]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  9 11 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "2022-11-24 06:41:51\n",
      "50000 Saving results\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  3  4  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "2022-11-24 09:48:49\n",
      "60000 Saving results\n",
      "0.09999999999999992 [0 1 2 3 4 5 6 7 8 9]\n",
      "0.09999999999999992 [ 1  2  3  4  5  6  7  8  9 10 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "0.09999999999999992 [ 0  1  2  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  2  3  6  7  9 11 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "2022-11-24 12:56:16\n",
      "70000 Saving results\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "2022-11-24 15:57:08\n",
      "80000 Saving results\n",
      "0.09999999999999992 [ 2  4 10 12]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "0.09999999999999992 [ 0  1  3  5  6  7  9 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "0.09999999999999992 [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14]\n",
      "2022-11-24 19:03:28\n",
      "training done!\n",
      "training done !!\n"
     ]
    }
   ],
   "source": [
    "df = df_train\n",
    "spea_action = df[\"rate_bin\"].unique().tolist()\n",
    "\n",
    "geometric_sampling = True\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "reg_lambda = 5\n",
    "#per_flag = True # per_flag 是决定是否根据优先度权重构建样本\n",
    "clip_reward = True\n",
    "\n",
    "# The main training loop is here\n",
    "batch_size = 256\n",
    "tau = 0.05 #Rate to update target network toward primary network\n",
    "gamma = 0.99 # discount factor  # 只考虑后面的两步，更注重当下利益\n",
    "\n",
    "load_model = False # Whether to load a saved model.\n",
    "save_dir = \"./dqn_normal_noterm_R8_v1_t1/\"\n",
    "save_path = \"./dqn_normal_noterm_R8_v1_t1/ckpt\" #The path to save our model to.\n",
    "epsilon = 0.9 # epsilon_greedy\n",
    "bak_epsilon = epsilon\n",
    "num_steps = 90000 # How many steps to train for\n",
    "iternum = 10 \n",
    "\n",
    "mainQN = Qnetwork()\n",
    "targetQN = Qnetwork()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables()) \n",
    "init = tf.global_variables_initializer() \n",
    "trainables = tf.trainable_variables() # 查看模型训练过程中的一些参数\n",
    "target_ops = update_target_graph(trainables,tau)\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    net_loss = 0.0\n",
    "    for i in range(num_steps):\n",
    "        probs = np.array(random.random())        \n",
    "        states,actions,rewards,next_states, sampled_df = process_train_batch(batch_size, df)\n",
    "        actions_min = actions - 4\n",
    "        actions_min[actions_min<=0] = 0\n",
    "        actions_max = actions + 4\n",
    "        actions_max[actions_max>=max_rate_bin] = max_rate_bin\n",
    "        # firstly get the chosen actions at the next timestep\n",
    "        if probs > epsilon:\n",
    "            actions_from_q1 = sess.run(mainQN.predict_fix, feed_dict={mainQN.state:next_states,  mainQN.phase : 1})\n",
    "            cur_act = sess.run(mainQN.predict_fix, feed_dict={mainQN.state:states, mainQN.phase : 1})\n",
    "        else:\n",
    "            actions_from_q1 = sess.run(mainQN.predict_ran, feed_dict={mainQN.state:next_states, mainQN.actions_min:actions_min, mainQN.actions_max:actions_max,mainQN.phase : 1})\n",
    "            cur_act = sess.run(mainQN.predict_ran, feed_dict={mainQN.state:states, mainQN.actions_min:actions_min, mainQN.actions_max:actions_max, mainQN.phase : 1})\n",
    "#        print(\"predict:\",actions_from_q1)\n",
    "        # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "        Q1 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "        for j in range(1,iternum,1): \n",
    "            Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "            Q1 = np.dstack((Q1, Q2))\n",
    "        Q2 = np.mean(Q1, axis=2)\n",
    "\n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(len(Q2)), actions_from_q1]\n",
    "\n",
    "        # empirical hack to make the Q values never exceed the threshold - helps learning\n",
    "      #  double_q_value[double_q_value > REWARD_THRESHOLD_max] = REWARD_THRESHOLD_max\n",
    "      #  double_q_value[double_q_value < REWARD_THRESHOLD_min] = REWARD_THRESHOLD_min\n",
    "        \n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value) \n",
    "      \n",
    "        # Train with the batch\n",
    "        _,loss, error = sess.run([mainQN.update_model, mainQN.loss, mainQN.abs_error], \\\n",
    "            feed_dict={mainQN.state:states,\n",
    "                       mainQN.targetQ:targetQ, \n",
    "                       mainQN.actions:cur_act,\n",
    "                       mainQN.phase:True})\n",
    "\n",
    "        # Update target towards main network\n",
    "        update_target(target_ops,sess) \n",
    "        net_loss += sum(error)\n",
    "        \n",
    "        if epsilon > 0.1 and (i % 1000 == 900):\n",
    "            epsilon = 0.1\n",
    "        if i % 1000 ==0:\n",
    "            ran_q, phys_q, phys_actions, agent_q, agent_actions, mean_abs_error = do_eval(batch_size = 100, eval_type = 'val')\n",
    "            epsilon = bak_epsilon\n",
    "            print(epsilon, np.unique(agent_actions))\n",
    "            \n",
    "            \n",
    "            if (i % 10000==0) and (i >= 10000):\n",
    "                if epsilon > 0.1:\n",
    "                    epsilon = epsilon - 0.2\n",
    "                    bak_epsilon = epsilon\n",
    "                if epsilon <= 0.1:\n",
    "                     epsilon = 0.1\n",
    "                epsilon = np.array(epsilon)\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "                print (i, \"Saving results\")\n",
    "                saver.save(sess,save_path)\n",
    "                do_save_results()\n",
    "    saver.save(sess,save_path)\n",
    "    do_save_results()\n",
    "print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "print(\"training done!\") \n",
    "\n",
    "filename = r'model_para.pkl'\n",
    "save_dir2  =  save_dir + filename\n",
    "save_lst = [\"save_dir\", \"spea_action\",\"geometric_sampling\", \"reg_lambda\", \"batch_size\", \"tau\", \"gamma\", \"epsilon\", \"num_steps\" ]\n",
    "my_save(save_lst, save_dir2, globals())\n",
    "print(\"training done !!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对性的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T03:42:27.441311Z",
     "start_time": "2022-11-30T01:54:39.309920Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 09:54:39\n",
      "Trying to load model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn_normal_noterm_R8_v1_t2/ckpt\n",
      "Model restored\n",
      "Init done\n",
      "0.3 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "0.3 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "0.3 [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14]\n",
      "2022-11-30 10:32:19\n",
      "2000 Saving results\n",
      "0.09999999999999998 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14]\n",
      "0.09999999999999998 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "2022-11-30 11:14:13\n",
      "4000 Saving results\n",
      "2022-11-30 11:42:27\n",
      "training done!\n",
      "training done !!\n"
     ]
    }
   ],
   "source": [
    "spea_action = [0,1,2,3,4,5,6,7,8,9,10,11] \n",
    "df = df_train[df_train[\"rate_bin\"].isin(spea_action)]\n",
    "\n",
    "geometric_sampling = True\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "reg_lambda = 5\n",
    "#per_flag = True # per_flag 是决定是否根据优先度权重构建样本\n",
    "clip_reward = True\n",
    "\n",
    "# The main training loop is here\n",
    "batch_size = 256\n",
    "tau = 0.05 #Rate to update target network toward primary network\n",
    "gamma = 0.5 # discount factor  # 只考虑后面的两步，更注重当下利益\n",
    "\n",
    "load_model = True # ther to load a saved model.\n",
    "save_dir = \"./dqn_normal_noterm_R8_v1_t2/\"\n",
    "save_path = \"./dqn_normal_noterm_R8_v1_t2/ckpt\" #The path to save our model to.\n",
    "epsilon = 0.3 # epsilon_greedy\n",
    "bak_epsilon = epsilon\n",
    "num_steps = 10000 # How many steps to train for\n",
    "iternum = 10 \n",
    "\n",
    "mainQN = Qnetwork()\n",
    "targetQN = Qnetwork()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables()) \n",
    "init = tf.global_variables_initializer() \n",
    "trainables = tf.trainable_variables() # 查看模型训练过程中的一些参数\n",
    "target_ops = update_target_graph(trainables,tau)\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    net_loss = 0.0\n",
    "    for i in range(num_steps):\n",
    "        probs = np.array(random.random())        \n",
    "        states,actions,rewards,next_states, sampled_df = process_train_batch(batch_size, df)\n",
    "        actions_min = actions - 3\n",
    "        actions_min[actions_min<=0] = 0\n",
    "        actions_max = actions + 3\n",
    "        actions_max[actions_max>=max_rate_bin] = max_rate_bin\n",
    "        # firstly get the chosen actions at the next timestep\n",
    "        if probs > epsilon:\n",
    "            actions_from_q1 = sess.run(mainQN.predict_fix, feed_dict={mainQN.state:next_states,  mainQN.phase : 1})\n",
    "            cur_act = sess.run(mainQN.predict_fix, feed_dict={mainQN.state:states, mainQN.phase : 1})\n",
    "        else:\n",
    "            actions_from_q1 = sess.run(mainQN.predict_ran, feed_dict={mainQN.state:next_states, mainQN.actions_min:actions_min, mainQN.actions_max:actions_max,mainQN.phase : 1})\n",
    "            cur_act = sess.run(mainQN.predict_ran, feed_dict={mainQN.state:states, mainQN.actions_min:actions_min, mainQN.actions_max:actions_max, mainQN.phase : 1})\n",
    "\n",
    "        # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "        Q1 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "        for j in range(1,iternum,1): \n",
    "            Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "            Q1 = np.dstack((Q1, Q2))\n",
    "        Q2 = np.mean(Q1, axis=2)\n",
    "\n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(len(Q2)), actions_from_q1]\n",
    "\n",
    "        # empirical hack to make the Q values never exceed the threshold - helps learning\n",
    "      #  double_q_value[double_q_value > REWARD_THRESHOLD_max] = REWARD_THRESHOLD_max\n",
    "      #  double_q_value[double_q_value < REWARD_THRESHOLD_min] = REWARD_THRESHOLD_min\n",
    "        \n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value) \n",
    "      \n",
    "        # Train with the batch\n",
    "        _,loss, error = sess.run([mainQN.update_model, mainQN.loss, mainQN.abs_error], \\\n",
    "            feed_dict={mainQN.state:states,\n",
    "                       mainQN.targetQ:targetQ, \n",
    "                       mainQN.actions:cur_act,\n",
    "                       mainQN.phase:True})\n",
    "\n",
    "        # Update target towards main network\n",
    "        update_target(target_ops,sess) \n",
    "        net_loss += sum(error)\n",
    "        \n",
    "        if epsilon > 0.1 and (i % 1000 == 900):\n",
    "            epsilon = 0.1\n",
    "        if i % 1000 ==0:\n",
    "            ran_q, phys_q, phys_actions, agent_q, agent_actions, mean_abs_error = do_eval(batch_size = 100, eval_type = 'val')\n",
    "            epsilon = bak_epsilon\n",
    "            print(epsilon, np.unique(agent_actions))\n",
    "            \n",
    "            if (i % 2000==0) and (i >= 2000):\n",
    "                if epsilon > 0.1:\n",
    "                    epsilon = epsilon - 0.2\n",
    "                    bak_epsilon = epsilon\n",
    "                if epsilon <= 0.1:\n",
    "                     epsilon = 0.1\n",
    "                epsilon = np.array(epsilon)\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "                print (i, \"Saving results\")\n",
    "                saver.save(sess,save_path)\n",
    "                do_save_results()\n",
    "    saver.save(sess,save_path)\n",
    "    do_save_results()\n",
    "print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "print(\"training done!\") \n",
    "\n",
    "filename = r'model_para.pkl'\n",
    "save_dir2  =  save_dir + filename\n",
    "save_lst = [\"save_dir\", \"spea_action\",\"geometric_sampling\", \"reg_lambda\", \"batch_size\", \"tau\", \"gamma\", \"epsilon\", \"num_steps\" ]\n",
    "my_save(save_lst, save_dir2, globals())\n",
    "print(\"training done !!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T11:03:28.400968Z",
     "start_time": "2022-11-24T11:03:28.349671Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nload_model = True\\nwith tf.Session(config=config) as sess:\\n    if load_model == True:\\n        print(\\'Trying to load model...\\')\\n        try:\\n            restorer = tf.train.import_meta_graph(save_path + \\'.meta\\')\\n            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\\n            print (\"Model restored\")\\n        except IOError:\\n            print (\"No previous model found, running default init\")\\n            sess.run(init)\\n    else:\\n        print(\"Running default init\")\\n        sess.run(init)\\n    print(\"Init done\")\\n    do_save_results()\\nprint(\"training done!\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save_path = \"./dqn_normal_noterm_stability_test/\"\n",
    "'''\n",
    "load_model = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    do_save_results()\n",
    "print(\"training done!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evalution for model's convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随着epoch的增加loss 的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T11:03:28.404148Z",
     "start_time": "2022-11-24T11:03:28.401975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.plot(av_loss_step, av_loss_lst)\\nplt.xlabel(\"sample size\")\\nplt.ylabel(\"loss\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.plot(av_loss_step, av_loss_lst)\n",
    "plt.xlabel(\"sample size\")\n",
    "plt.ylabel(\"loss\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随着epoch的增加平均Q打分\n",
    "- 这里认为当模型对推荐的action的平均Q打分 比较稳定且高时，认为模型基本收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T11:03:28.407518Z",
     "start_time": "2022-11-24T11:03:28.405094Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'av_q_step = np.array(av_q_step)\\nav_agent_q_lst = np.array(av_agent_q_lst)\\nav_agent_q_std_lst = np.array(av_agent_q_std_lst)\\nav_phys_q_lst = np.array(av_phys_q_lst)\\nav_phys_q_std_lst = np.array(av_phys_q_std_lst)\\nav_ran_q_lst = np.array(av_ran_q_lst)\\nav_ran_q_std_lst = np.array(av_ran_q_std_lst)\\n\\nplt.figure(figsize=(6, 4.5))\\n\\nplt.plot(av_q_step, av_agent_q_lst, color=\\'#74c476\\') # \\nplt.plot(av_q_step, av_phys_q_lst, color=\\'#c51b8a\\')\\nplt.plot(av_q_step, av_ran_q_lst, color=\\'#3182bd\\')\\n\\nplt.fill_between(av_q_step, av_agent_q_lst - 1*av_agent_q_std_lst, av_agent_q_lst + 1*av_agent_q_std_lst, color=\\'#a1d76a\\',alpha = 0.7)\\nplt.fill_between(av_q_step, av_phys_q_lst - 1*av_phys_q_std_lst, av_phys_q_lst + 1*av_phys_q_std_lst, color=\\'#e9a3c9\\', alpha = 0.5) #phy\\nplt.fill_between(av_q_step, av_ran_q_lst - 1*av_ran_q_std_lst, av_ran_q_lst + 1*av_ran_q_std_lst, color=\\'#9ecae1\\', alpha = 0.2)\\nplt.legend([\"agent\", \"phy\",\"random_action\"])\\n\\nplt.xlabel(\"sample size\")\\nplt.ylabel(\"Q\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''av_q_step = np.array(av_q_step)\n",
    "av_agent_q_lst = np.array(av_agent_q_lst)\n",
    "av_agent_q_std_lst = np.array(av_agent_q_std_lst)\n",
    "av_phys_q_lst = np.array(av_phys_q_lst)\n",
    "av_phys_q_std_lst = np.array(av_phys_q_std_lst)\n",
    "av_ran_q_lst = np.array(av_ran_q_lst)\n",
    "av_ran_q_std_lst = np.array(av_ran_q_std_lst)\n",
    "\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "\n",
    "plt.plot(av_q_step, av_agent_q_lst, color='#74c476') # \n",
    "plt.plot(av_q_step, av_phys_q_lst, color='#c51b8a')\n",
    "plt.plot(av_q_step, av_ran_q_lst, color='#3182bd')\n",
    "\n",
    "plt.fill_between(av_q_step, av_agent_q_lst - 1*av_agent_q_std_lst, av_agent_q_lst + 1*av_agent_q_std_lst, color='#a1d76a',alpha = 0.7)\n",
    "plt.fill_between(av_q_step, av_phys_q_lst - 1*av_phys_q_std_lst, av_phys_q_lst + 1*av_phys_q_std_lst, color='#e9a3c9', alpha = 0.5) #phy\n",
    "plt.fill_between(av_q_step, av_ran_q_lst - 1*av_ran_q_std_lst, av_ran_q_lst + 1*av_ran_q_std_lst, color='#9ecae1', alpha = 0.2)\n",
    "plt.legend([\"agent\", \"phy\",\"random_action\"])\n",
    "\n",
    "plt.xlabel(\"sample size\")\n",
    "plt.ylabel(\"Q\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随着epoch的增加两者行为差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T11:03:28.410671Z",
     "start_time": "2022-11-24T11:03:28.408466Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure(figsize=(6, 4.5))\\nplt.plot(av_q_step, M_phy_ac, color=\\'#7fc97f\\')\\nplt.plot(av_q_step, E_phy_ac, color=\\'#beaed4\\')\\nplt.plot(av_q_step, L_phy_ac, color=\\'#fdc086\\')\\nplt.plot(av_q_step, EA_phy_ac, color=\\'#3182bd\\')\\n\\nplt.legend([\"ac_prec_M_phy\",\"ac_prec_E_phy\",\"ac_prec_L_phy\", \"ac_prec_EA_phy\"])'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.figure(figsize=(6, 4.5))\n",
    "plt.plot(av_q_step, M_phy_ac, color='#7fc97f')\n",
    "plt.plot(av_q_step, E_phy_ac, color='#beaed4')\n",
    "plt.plot(av_q_step, L_phy_ac, color='#fdc086')\n",
    "plt.plot(av_q_step, EA_phy_ac, color='#3182bd')\n",
    "\n",
    "plt.legend([\"ac_prec_M_phy\",\"ac_prec_E_phy\",\"ac_prec_L_phy\", \"ac_prec_EA_phy\"])'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mytensor]",
   "language": "python",
   "name": "conda-env-mytensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
